qplot(mtcars$am, mtcars$mpg, geom = "boxplot", col = c("red", "blue"))
qplot(mtcars$am, mtcars$mpg, geom = "boxplot", aes(x = "red", y = "green")
)
qplot(mtcars$am, mtcars$mpg, geom = "boxplot")
qplot(mtcars$am, mtcars$mpg, geom = "boxplot", xlab = "Automatic/Manual", ylab = "MPG")
split(mtcars, mtcars$am)
automan <- split(mtcars, am)
automan <- split(mtcars, mtcars$am)
lapply(automan, summary)
lapply(mtcars, function(x) summary(x$mpg))
lapply(mtcars, function(x) summary(x[mpg]))
lapply(mtcars, function(x) summary(x["mpg"]))
lapply(mtcars, function(x) summary(x[1]))
lapply(mtcars, function(x) summary(x[[1]]))
lapply(mtcars, function(x) summary(x[, 1]))
lapply(mtcars, function(x) summary(x[1,]))
lapply(mtcars, function(x) summary(x[[1,]]))
lapply(mtcars, function(x) summary(x[[,1]]))
lapply(automan, function(x) summary(x[[1]]))
lapply(automan, function(x) summary(x[1]))
lapply(automan$mpg, mean)
automan
lapply(automan, function(x) mean(x[1]))
lapply(automan, function(x) mean(x[2]))
lapply(automan, function(x) mean(x[[2]]))
lapply(automan, function(x) summary(x[1]))
knitr::opts_chunk$set(echo = TRUE)
rapply(automan, function(x) mean(x[2]))
sapply(automan, function(x) mean(x[2]))
lapply(automan, function(x) mean(x[1]))
automan
lapply(automan, function(x) mean(x[1]))
automan$Automatic
x <- automan$Automatic
mean(x$mpg)
lapply(automan, function(x) mean(x$mpg))
lapply(automan, function(x) mean(x$mpg))
x <- automan$Automatic
y <- automan$Manual
t.test(x$mpg, y$mpg)
automan <- split(mtcars, am)
init <- lm(mpg ~ am, data = mtcars)
summary(init)
init <- lm(mpg ~ am, data = mtcars)
bestmodel <- step(init, direction = "both")
summary(bestmodel)
init <- lm(mpg ~ ., data = mtcars)
bestmodel <- step(init, direction = "both")
summary(bestmodel)
init2 <- lm(mpg ~ am, data = mtcars)
anova(init2, bestmodel)
par(mfrow = c(2,2))
plot(bestmodel)
plot(bestmodel)#
# install package and set data frame
if (!require(kernlab)){
install.packages("kernlab")
library(kernlab)
}
# install package and set data frame
if (!require(kernlab)){
install.packages("kernlab")
library(kernlab)
} else {
library(kernlab)
}
# set data frame
data("spam")
# vieww data
view(head(spam, 100))
# vieww data
View(head(spam, 100))
# take a very small sample for example purpose
smallSpam <- spam[sample(dim(spam)[1], size = 10),]
smallSpam
dim(spam)[1]
spamLabel <- (smallSpam$type == "spam")*1 + 1
set.seed(333)
# take a very small sample for example purpose
smallSpam <- spam[sample(dim(spam)[1], size = 10),]
spamLabel <- (smallSpam$type == "spam")*1 + 1
# plot
plot(smallSpam$capitalAve, col=spamLabel)
# plot
plot(smallSpam$capitalAve, col=spamLabel, ylab = "number of capital letters",
xlab = "Email Index")
# Apply rule 1 to smallSpam
rule1 <- function(x) {
prediction <- rep(NA, length(x))
prediction[x > 2.7] <- "spam"
prediction[x < 2.4] <- "nonspam"
prediction[(x >= 2.4 & x <= 2.45)] <- "spam"
prediction[(x > 2.45 & x <= 2.7)] <- "nonspam"
return(prediction)
}
table(rule1(smallSpam$capitalAve), smallSpam$type)
# Apply rule 2 to smallSpam
rule2 <- function(x) {
prediction <- rep(NA, length(x))
prediction[x > 2.4] <- "spam"
prediction[x <= 2.4] <- "nonspam"
return(prediction)
}
# Results
table(rule2(smallSpam$capitalAve), smallSpam$type)
# Apply rule 2 to smallSpam
rule2 <- function(x) {
prediction <- rep(NA, length(x))
prediction[x > 2.8] <- "spam"
prediction[x <= 2.8] <- "nonspam"
return(prediction)
}
# Results
table(rule2(smallSpam$capitalAve), smallSpam$type)
# Apply rule 2 to smallSpam
rule2 <- function(x) {
prediction <- rep(NA, length(x))
prediction[x > 2.7] <- "spam"
prediction[x <= 2.7] <- "nonspam"
return(prediction)
}
# Results
table(rule2(smallSpam$capitalAve), smallSpam$type)
# Now we apply these two rules to the complete spam dataset
table(rule1(spam$capitalAve), spam$type)
table(rule2(spam$capitalAve), spam$type)
mean(rule1(spam$capitalAve) == spam$type)
mean(rule2(spam$capitalAve) == spam$type)
sum(rule2(spam$capitalAve) == spam$type)
install.packages("caret")
if (!require(caret)){
install.packages("caret")
library(caret)
} else {
library(caret)
}
if (!require(caret)){
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
library(caret)
} else {
library(caret)
}
rm(list = ls())
#####
# install package
if (!require(kernlab)){
install.packages("kernlab")
library(kernlab)
}
if (!require(caret)){
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
library(caret)
}
# import dataset and seed (To get the same result every time)
data("spam")
set.seed(32343)
# flag a training set with a random sample of 75% from the total
# population
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
# Split the population with the flag
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
# Fit a model
modelFit <- train(type ~., data = training, method = "glm")
install.packages("e1071")
library(e1071)
# Fit a model
# WARNING: might need to download and install package e1071!
modelFit <- train(type ~., data = training, method = "glm")
warnings()
modelFit
# Final model
modelFit$finalModel
# Create Predictions Based On Linear Regression
predictions <- predict(modelFit, newdata = testing)
# Confusion Matrix
confusionMatrix(predictions, testing$type)
?createFolds
time <- 1:1000
folds <- createTimeSlices(y = time, initialWindow = 20, horizon = 10)
# Returns training set
fold$train[[1]]
# Returns training set
folds$train[[1]]
# Returns test set
folds$test[[1]]
#####
# install package
if (!require(ISLR)){
install.packages("ISLR")
library(ISLR)
}
if (!require(caret)){
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
library(caret)
}
# import dataset and seed (To get the same result every time)
data("Wage")
rm(list = ls())
# import dataset and seed (To get the same result every time)
data("Wage")
# view data
View(head(Wage, 100))
Summary(Wage)
summary(Wage)
library("caret", lib.loc="~/R/win-library/3.4")
library("e1071", lib.loc="~/R/win-library/3.4")
library("ggplot2", lib.loc="~/R/win-library/3.4")
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x = training[, c("age", "education", "jobclass")],
y = training$wage,
plot = "pairs")
qplot(age, wage, data = training)
qplot(age, wage, colour = jobclass, data = training)
qq <- qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = 'lm', formula = y~x)
if (!require(Hmisc)){
install.packages("Hmisc")
library(Hmisc)
}
cutWage <- cut2(training$wage, g=3)
table(cutWage)
p1 <- qplot(cutWage, age, data = training, fill = cutWage,
geom=c("boxplot"))
p1
grid.arrange(p1,p2,ncol=2)
par(mfrow = c(1,2))
p1
p2
p2 <- qplot(cutWage, age, data = training, fill = cutWage,
geom = c("boxplot", "jitter"))
p2
par(mfrow = c(1,2))
p1
p2
par(mfrow = c(1,2))
p1
p2
par(mfrow = c(2,1))
p1
p2
par(mfrow = c(2,1))
plot(p1)
plot(p2)
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1)
# looking at proportions by row - insert 2 instead of 1 implies by column
prop.table(t1, 2)
# looking at proportions by row - insert 2 instead of 1 implies by column
prop.table(t1, 1)
# Looking at density plots
qplot(wage, colour = education, data = training, geom = "density")
#####
# install package
if (!require(kernlab)){
install.packages("kernlab")
library(kernlab)
}
if (!require(caret)){
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
library(caret)
}
# import dataset and seed (To get the same result every time)
data("spam")
# view data
View(head(spam, 100))
summary(spam)
# Spliting Data
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
# Check skewed data
hist(training$capitalAve, main = "")
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
# to standardize the data..
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
# we also need to standardize the test set with mean and sd of training set
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
# check hist and qq-plot
hist(trainCapAveS)
# check hist and qq-plot
hist(trainCapAveS)
qqnorm(trainCapAveS)
mean(trainCapAveS)
sd(trainCapAveS)
View(trainCapAveS)
# Spliting Data
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
# Check skewed data
hist(training$capitalAve, main = "")
# to standardize the data.. first standardize the training set
rainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
# we also need to standardize the test set with mean and sd of training set
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
# check hist and qq-plot
hist(trainCapAveS)
qqnorm(trainCapAveS)
View(trainCapAveS)
View(trainCapAve)
(1102.50 - mean(trainCapAve))/sd(trainCapAve)
# install package
if (!require(ISLR)){
install.packages("ISLR")
library(ISLR)
}
if (!require(caret)){
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
library(caret)
}
data("Wage")
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
table(training$jobclass)
# creating dummy variables - i.e. turning character variable to 1 or 0
dummies <- dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
# removing zero covariates
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
# example
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = "red",
pch = 19, cex = 0.5)
predict(bsBasis, age = testing$age)
?set.seed
if (!require(caret)){
install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
library(caret)
}
data("faithful")
set.seed(333)
inTrain <- createDataPartition(y = faithful$waiting), p = 0.5, list = FALSE)
trainFaith <- faithful[inTrain,]
inTrain <- createDataPartition(y = faithful$waiting, p = 0.5, list = FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue",
xlab = "Waiting", ylab = "Duration")
# Fit a linear model
lm1 <- lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
#
lines(trainFaith$waiting, lm1$fitted, lwd = 3)
# Predict
predict(lm1, waitingTime)
# Set Waiting Time we want to predict
waitingTime <- data.frame(waiting=80)
# Predict
predict(lm1, waitingTime)
View(trainFaith)
testPlot <- plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "red",
xlab = "Waiting", ylab = "Duration")
testLine <- lines(testFaith$waiting, lm1$fitted, lwd = 3)
par(mfrow = c(1,2))
trainPlot; trainLine
testPlot; testLine
trainPlot <- plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue",
xlab = "Waiting", ylab = "Duration")
trainPlot
par(mfrow = c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue",
xlab = "Waiting", ylab = "Duration")
lines(trainFaith$waiting, predict(lm1), lwd(3))
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "red",
xlab = "Waiting", ylab = "Duration")
lines(testFaith$waiting, predict(lm1, newdata = testFaith, lwd = 3))
lines(trainFaith$waiting, predict(lm1, newdata = trainFaith), lwd(3))
lines(trainFaith$waiting, predict(lm1), lwd = 3)
par(mfrow = c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue",
xlab = "Waiting", ylab = "Duration")
lines(trainFaith$waiting, predict(lm1), lwd = 3)
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "red",
xlab = "Waiting", ylab = "Duration")
lines(testFaith$waiting, predict(lm1, newdata = testFaith, lwd = 3))
par(mfrow = c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue",
xlab = "Waiting", ylab = "Duration", main = "Training")
lines(trainFaith$waiting, predict(lm1), lwd = 3)
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "red",
xlab = "Waiting", ylab = "Duration", main = "Test")
lines(testFaith$waiting, predict(lm1, newdata = testFaith, lwd = 3))
# Calculate RMSE on training
sqrt(sum((lm1$fitted.values-trainFaith$eruptions)^2))
# Calculate RMSE on test
sqrt(sum((predict(lm1, newdata = testFaith)-testFaith$eruptions)^2))
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
rm(list = ls())
data(AlzheimerDisease)
rm(list = ;s())
rm(list = l())
rm(list = ls())
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
if (!require(caret)){
install.packages("caret")
library(caret)
}
if (!require(rpart)){
install.packages("rpart")
library(rpart)
}
if (!require(rpart.plot)){
install.packages("rpart.plot")
library(rpart.plot)
}
if (!require(randomForest)){
install.packages("randomForest")
library(randomForest)
}
if (!require(rattle)){
install.packages("rattle")
library(rattle)
}
setwd("C:/Users/osian/Desktop/Machine Learning/Project")
raw.test <- read.csv(file = "pml-testing.csv", header = T)
raw.training <- read.csv(file = "pml-training.csv", header = T)
View(head(raw.training, 1000))
dim(raw.training)
dim(raw.test)
# BASIC SUMMARIES AND BACKGROUND
summary(raw.training$classe) # outcome variable
class(raw.training$classe)
wip.training <- raw.training[-(1:7)]
wip.test <- raw.test[-(1:7)]
# Data Frame contains columns with many NAs
colSums(is.na(wip.training))
# Getting rid of columns containing NAs
wip.training <- wip.training[sapply(wip.training, function(x) !any(is.na(x)))]
wip.test <- wip.test[sapply(wip.test, function(x) !any(is.na(x)))]
# Check for zero variance or near zero variance
# This will get rid of constants and missing values
nzv1 <- nearZeroVar(wip.training, saveMetrics = T)
nzv2 <- nearZeroVar(wip.test, saveMetrics = T)
wip.training <- wip.training[, nzv1$nzv == FALSE]
wip.test <- wip.test[, nzv2$nzv == FALSE]
# finished dataset
train <- wip.training
validate <- wip.test
View(head(train))
View(head(validate))
# check the remaining variables
colnames(train)
set.seed(123)
part <- createDataPartition(y = train$classe, p = 0.75, list = FALSE)
train.subTrain <- train[part,]
train.subTest <- train[-part,]
Decision_Tree.Fit <- rpart(classe ~. , data = train.subTrain, method = "class")
fancyRpartPlot(Decision_Tree.Fit)
prediction1 <- predict(Decision_Tree.Fit, train.subTest, type = "class")
con.Matrix1 <- confusionMatrix(prediction1, train.subTest$classe)
con.Matrix1
modFit2 <- randomForest(classe ~ ., data = train.subTrain)
prediction2 <- predict(modFit2, train.subTest, type = "class")
con.Matrix2 <- confusionMatrix(prediction2, train.subTest$classe)
con.Matrix2
modFit <- randomForest(classe ~., data = train)
prediction <- predict(modFit, validate, type = "class")
con.Matrix <- confusionMatrix(prediction, validate$class)
summary(validate)
con.Matrix <- confusionMatrix(prediction, validate$classe)
prediction
as.character(prediction)
View(head(raw.training))
head(raw.test)
View(wip.training)
View(wip.training)
